{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d7ba314",
   "metadata": {},
   "source": [
    "1. import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc9d75b4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#import\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from boruta import BorutaPy\n",
    "import itertools\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f9909a",
   "metadata": {},
   "source": [
    "2. definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52be5312",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#def\n",
    "\n",
    "def yyplot_k(y_obs, y_est, y_obs_label = \"Observed Đ\", y_est_label = \"Predicted Đ\"):\n",
    "\n",
    "    plt.rcParams[\"font.size\"] = 20\n",
    "    plt.figure(figsize = (8,8))\n",
    "\n",
    "    plt.scatter(y_obs,y_est,alpha = 1)\n",
    "\n",
    "    ratio = 0.03\n",
    "    axv = plt.axis()\n",
    "    axrange = axv[1] - axv[0]\n",
    "    axmin = min(axv) - ratio*axrange\n",
    "    axmax = max(axv) + ratio*axrange\n",
    "    plt.axis([axmin,axmax,axmin,axmax])\n",
    "\n",
    "    plt.plot([axmin,axmax],[axmin,axmax],color = \"gray\")\n",
    "\n",
    "    plt.xlabel(y_obs_label)\n",
    "    plt.ylabel(y_est_label)\n",
    "    \n",
    "def yyplot_train_test(y_obs_train, y_est_train, y_obs_test, y_est_test, y_obs_label = \"Observed Đ\", y_est_label = \"Predicted Đ\"):\n",
    "\n",
    "    plt.rcParams[\"font.size\"] = 20\n",
    "    plt.figure(figsize = (8,8))\n",
    "\n",
    "    plt.scatter(y_obs_train,y_est_train,alpha = 1, color = \"blue\", label=\"training\")\n",
    "    plt.scatter(y_obs_test,y_est_test,alpha = 1, color = \"orange\", label=\"test\")\n",
    "\n",
    "    ratio = 0.03\n",
    "    axv = plt.axis()\n",
    "    axrange = axv[1] - axv[0]\n",
    "    axmin = min(axv) - ratio*axrange\n",
    "    axmax = max(axv) + ratio*axrange\n",
    "    plt.axis([axmin,axmax,axmin,axmax])\n",
    "\n",
    "    plt.plot([axmin,axmax],[axmin,axmax],color = \"gray\")\n",
    "\n",
    "    plt.xlabel(y_obs_label)\n",
    "    plt.ylabel(y_est_label)\n",
    "    \n",
    "def search_highly_correlated_variables(X, threshold_of_r):\n",
    "    r_in_x = X.corr()\n",
    "    r_in_x = abs(r_in_x)\n",
    "\n",
    "    for i in range(r_in_x.shape[0]):\n",
    "        r_in_x.iloc[i, i] = 0\n",
    "    highly_correlated_variable_numbers = []\n",
    "    \n",
    "    for i in range(r_in_x.shape[0]):\n",
    "        r_max = r_in_x.max()\n",
    "        r_max_max = r_max.max()\n",
    "        if r_max_max >= threshold_of_r:\n",
    "            variable_number_1 = np.where(r_max == r_max_max)[0][0]\n",
    "            variable_number_2 = np.where(r_in_x.iloc[:, variable_number_1] == r_max_max)[0][0]\n",
    "            r_sum_1 = r_in_x.iloc[:, variable_number_1].sum()\n",
    "            r_sum_2 = r_in_x.iloc[:, variable_number_2].sum()\n",
    "            if r_sum_1 >= r_sum_2:\n",
    "                delete_x_number = variable_number_1\n",
    "            else:\n",
    "                delete_x_number = variable_number_2\n",
    "            highly_correlated_variable_numbers.append(delete_x_number)\n",
    "            r_in_x.iloc[:, delete_x_number] = 0\n",
    "            r_in_x.iloc[delete_x_number, :] = 0\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    X.drop(X.columns[highly_correlated_variable_numbers], axis=1, inplace = True)\n",
    "    return X\n",
    "\n",
    "def search_highly_correlated_variables_cv(X_train, X_test, threshold_of_r):\n",
    "    r_in_x = X_train.corr()\n",
    "    r_in_x = abs(r_in_x)\n",
    "\n",
    "    for i in range(r_in_x.shape[0]):\n",
    "        r_in_x.iloc[i, i] = 0\n",
    "    highly_correlated_variable_numbers = []\n",
    "\n",
    "    for i in range(r_in_x.shape[0]):\n",
    "        r_max = r_in_x.max()\n",
    "        r_max_max = r_max.max()\n",
    "        if r_max_max >= threshold_of_r:\n",
    "            variable_number_1 = np.where(r_max == r_max_max)[0][0]\n",
    "            variable_number_2 = np.where(r_in_x.iloc[:, variable_number_1] == r_max_max)[0][0]\n",
    "            r_sum_1 = r_in_x.iloc[:, variable_number_1].sum()\n",
    "            r_sum_2 = r_in_x.iloc[:, variable_number_2].sum()\n",
    "            if r_sum_1 >= r_sum_2:\n",
    "                delete_x_number = variable_number_1\n",
    "            else:\n",
    "                delete_x_number = variable_number_2\n",
    "            highly_correlated_variable_numbers.append(delete_x_number)\n",
    "            r_in_x.iloc[:, delete_x_number] = 0\n",
    "            r_in_x.iloc[delete_x_number, :] = 0\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    X_train.drop(X_train.columns[highly_correlated_variable_numbers], axis=1, inplace = True)\n",
    "    X_test.drop(X_test.columns[highly_correlated_variable_numbers], axis=1, inplace = True)\n",
    "    return X_train, X_test\n",
    "\n",
    "def boruta(X, y, perc, rseed_boruta):\n",
    "    selected_columns_sum = []\n",
    "    for i in range(1):\n",
    "        base_bo = RandomForestRegressor(n_estimators = 40, max_depth = 8)\n",
    "        feat_selector = BorutaPy(base_bo, n_estimators = \"auto\", perc = perc, verbose =2, random_state=rseed_boruta)\n",
    "        feat_selector.fit(X.values, y.values)\n",
    "        selected = feat_selector.support_\n",
    "        selected_columns = X.columns[selected].to_list()\n",
    "        selected_columns_sum.append(selected_columns)\n",
    "\n",
    "    boruta = list(set(list(itertools.chain.from_iterable(selected_columns_sum))))\n",
    "    X = X[boruta]\n",
    "    return X\n",
    "\n",
    "def boruta_cv(X_train, y_train, X_test, perc, rseed_boruta):\n",
    "    selected_columns_sum = []\n",
    "    for i in range(1):\n",
    "        base_bo = RandomForestRegressor(n_estimators = 40, max_depth = 8)\n",
    "        feat_selector = BorutaPy(base_bo, n_estimators = \"auto\", perc = perc, verbose =2, random_state = rseed_boruta)\n",
    "        feat_selector.fit(X_train.values, y_train.values)\n",
    "        selected = feat_selector.support_\n",
    "        selected_columns = X_train.columns[selected].to_list()\n",
    "        selected_columns_sum.append(selected_columns)\n",
    "\n",
    "    boruta = list(set(list(itertools.chain.from_iterable(selected_columns_sum))))\n",
    "    X_train = X_train[boruta]\n",
    "    X_test = X_test[boruta]\n",
    "    return X_train, X_test\n",
    "\n",
    "def funcTanimoto_sklearn(a, b):\n",
    "    \"\"\"\n",
    "    Jaccard similarity is used as a kernel\n",
    "    \n",
    "    \"\"\"\n",
    "    if 0:\n",
    "        print(\"----\")\n",
    "        print(a.shape)\n",
    "        print(b.shape)\n",
    "        print(\"----\")\n",
    "        \n",
    "    if (a.ndim == 1) and (b.ndim == 1):\n",
    "        jdist = pairwise_distances(a.astype(bool, copy = False).reshape(1, -1), b.astype(bool, copy = False).reshape(1, -1), metric = \"jaccard\")\n",
    "    else:\n",
    "        jdist = pairwise_distances(a.astype(bool, copy = False), b.astype(bool, copy = False), metric = \"jaccard\")\n",
    "        \n",
    "    return 1 - jdist\n",
    "\n",
    "def T2_value(Xtr, Xts, dim):\n",
    "    \n",
    "    pca = PCA()\n",
    "    pca.fit(Xtr)\n",
    "    Xtr_pca = pca.transform(Xtr)\n",
    "    \n",
    "    # 寄与率（各主成分がどれだけ情報を保持するか）\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    # 累積寄与率（何次元まででどれだけカバーできるか）\n",
    "    cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "    \n",
    "    \n",
    "    Xts_pca = pca.transform(Xts)\n",
    "    \n",
    "    # print(pd.DataFrame(pca.explained_variance_ratio_))\n",
    "    \n",
    "    loading = pca.components_  # shape: (PC数, 変数数)\n",
    "    \n",
    "    Xtr_6 =  Xtr_pca[:, :dim]\n",
    "    Xts_6 =  Xts_pca[:, :dim]\n",
    "    \n",
    "    # 寄与度の計算：スコアとローディングから復元\n",
    "    contribution_vec = np.dot(Xts_6, loading[:dim,:])  # shape: (1, n_features)\n",
    "    contribution_squared = contribution_vec.squeeze()**2  # 二乗寄与度（各説明変数）\n",
    "    \n",
    "    varis = Xtr_6.var(axis=0)\n",
    "    T2_train_max = max(np.sum(np.square(Xtr_6)/varis, axis=1))\n",
    "    T2_test = np.sum(np.square(Xts_6)/varis, axis=1)\n",
    "\n",
    "    # 寄与度を表示\n",
    "    df_contrib = pd.DataFrame({\n",
    "        \"feature\": Xtr.columns,\n",
    "        \"contribution\": contribution_squared\n",
    "    }).sort_values(by=\"contribution\", ascending=False)\n",
    "    \n",
    "    return T2_test, T2_train_max, df_contrib, cumulative_explained_variance[dim]\n",
    "\n",
    "def knn(k, ratio, Xtr, Xts):\n",
    "    \n",
    "    # Construction and fitting of KNN models\n",
    "    ad_model = NearestNeighbors(n_neighbors=k + 1, metric='euclidean')\n",
    "    ad_model.fit(Xtr)\n",
    "    \n",
    "    # Calculate KNN distance for training data\n",
    "    knn_distance_train, _ = ad_model.kneighbors(Xtr)\n",
    "    mean_knn_distance_train = knn_distance_train[:, 1:].mean(axis=1)\n",
    "    \n",
    "    # Determination of AD thresholds\n",
    "    sorted_mean_knn_distance_train = np.sort(mean_knn_distance_train)\n",
    "    ad_threshold = sorted_mean_knn_distance_train[int(len(sorted_mean_knn_distance_train) * ratio) - 1]\n",
    "    \n",
    "    # Calculate KNN distance for test data\n",
    "    knn_distance_test, _ = ad_model.kneighbors(Xts)\n",
    "    mean_knn_distance_test = knn_distance_test[:, 1:].mean(axis=1)\n",
    "    \n",
    "    return mean_knn_distance_test, ad_threshold\n",
    "\n",
    "def RMSE (y, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    return rmse\n",
    "\n",
    "def MAE (y, y_pred):\n",
    "    mae = np.sqrt(mean_absolute_error(y, y_pred))\n",
    "    return mae\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
